
QUESTION:

1 How many floating operations are being performed in your matrix multiply kernel? explain.
  Assuming only the operatoins for the actual matrix multiply are considered
  floating point. Each thread will perform k * 2 floating point operations. K
  for the number of columns in A and rows in B , and 2 because of the
  multiplication and addition needed for each matrix entry. Since there will be 
  # Threads = BLOCK_SIZE * BLOCK_SIZE * TILE_SIZE * TILE_SIZE. 
  Total floating point ops = # Threads * k * 2.

2 How many global memory reads are being performed by your kernel? explain.
  So each thread will read (k - 1) / TILE_SIZE + 1 entries from both the A and
  B matrices. So total global mem reads = # Threads * 2 * (k - 1) / TILE_SIZE + 1

3 How many global memory writes are being performed by your kernel? explain.
  Global memory is only written once per thread given the row and col are in
  range, the writing of pvalue to the C matrix. So total num global mem writes
  is the size of the C matrix.

4 Explain why, given your answers to the first three questions, why your lab3 solution should be faster than your lab 2 solution.
  The lab3 solution should be faster because the number of global memory reads
  per thread is significantly reduced. We now use a tiled version so each thread
  does part of the work of reading from global memory instead of having each
  thread do all the global memory reads required to calculate the value. This
  is accomplished by using shared memory between the threads in the current
  block.

5 In your kernel implementation, how many threads can be simultaneously scheduled for execution on a GeForce GTX 280 GPU, which contains 30 streaming multiprocessors? Use:

    nvcc --ptxas-options="-v" kernel.cu

to see the resource usage of your kernel (although compilation will fail, it will only do so after compiling the kernel and displaying the relevant information). Show how you calculated your answer keeping in mind the various relevant constraints.

Use http://en.wikipedia.org/wiki/CUDA to see the resource information for GTX280 GPU. Check the compute capability for GTX280 and then check the technical specifications of it. 

ANSWER: So the GTX280 has a compute compatability versoin 1.3. It says that
there are 8 resident blocks per multiprocessor. Each of our blocks has 16 * 16
threads (256 threads). Each block uses 2048 bytes of smem and there's 16KB of
smem so we can run 8 blocks at a time per SM. However, each SM can only have
1024 resident threads which is greater than 8 * 256. So we can schedule 1024 *
30 threads at a time which equals 30,720.















